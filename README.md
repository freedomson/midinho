



# ğŸ›¡ï¸ Private Ollama: The 100% Private LLM Chatbot

A **fully offline**, secure, and open-source chatbot built with:

- ğŸ”§ **Ollama** (local LLM engine)
- ğŸ§± **Lit** (web components for modern UI)
- ğŸ§  **Langchain + Pyodide** (LLM orchestration in WebAssembly)
\
\
ğŸ‘‰ [ğŸ”— Live Demo](https://freedomson.github.io/midinho/)

![My Ad](screenshot.png)
---
## ğŸš€ Project Overview

**Private Ollama** is a client-side chatbot application that leverages modern web and AI technology to provide:

- âœ… Full **local execution** of LLM inference  
- âœ… Zero data leakage â€“ no internet access required post-load  
- âœ… Cross-platform browser support  
- âœ… Extendability with WebAssembly-based Python (Pyodide)

---
## ğŸ§© Tech Stack Breakdown

| Component      | Technology                                     | Role                                                   |
|----------------|------------------------------------------------|--------------------------------------------------------|
| ğŸ§  LLM Engine   | [Ollama](https://ollama.com)                  | Serves LLM models locally (e.g., Mistral, LLaMA, etc.) |
| âš›ï¸ Frontend UI  | [Lit](https://lit.dev)                        | Creates modern, reactive web components                |
| ğŸ”— LLM Pipeline | [Langchain](https://www.langchain.com) + [Pyodide](https://pyodide.org) | Enables Python-based logic and toolchains in-browser   |
| ğŸŒ Runtime      | Pyodide + WebAssembly                          | Runs Python Langchain in the browser, no server needed |

---

## ğŸ”’ Privacy & Offline Capabilities

### ğŸ” Works Fully Offline

- Once loaded in the browser, **no internet is required** to:
  - Run the chatbot  
  - Query the model  
  - Process prompts or tools  

### ğŸ” 100% Local Data Flow

- No telemetry, logging, or external API calls
- Everything runs:
  - In your browser (UI + logic)
  - On your machine (model served locally via Ollama)

### ğŸ”‹ Use Cases

- Secure research  
- Education in remote/offline environments  
- Local-only enterprise chatbots  
- LLM experimentation sandbox  

---

## ğŸ–¼ï¸ App Architecture

```
+---------------------------+
|        Browser UI         |
| (Lit Web Components)      |
+------------+--------------+
             |
             v
+---------------------------+
|   Pyodide (Python WASM)   |
|  + Langchain Orchestration|
+------------+--------------+
             |
             v
+---------------------------+
|     Ollama Local Engine   |
|   (Runs on localhost:11434)|
+---------------------------+
```
---

## ğŸ“¦ Run on the internet

1. **Start Ollama locally** on your machine \
    [Download and install Ollama locally](https://ollama.com/download)

   ```bash
      # Run llama3
      ollama run llama3
      # Test status
      curl http://localhost:11434
      Ollama is running%
2. **ğŸ§ª Open Live Example UI** \
ğŸ‘‰ [https://freedomson.github.io/midinho/](https://freedomson.github.io/midinho/)

- No backend required
- Loads Pyodide + Langchain in-browser
- Connects to local Ollama for LLM completions


## ğŸ“¦ Run on your computer

1. **Start Ollama Locally** on your machine \
    [Download and install Ollama locally](https://ollama.com/download)

   ```bash
      # Run llama3
      ollama run llama3
      # Test status
      curl http://localhost:11434
      Ollama is running%
2. **Serve Live LLM Chat Bot with Python**
   ```bash
      cd static
      python server.py
2. **ğŸ§ª Open Live Example UI** \
ğŸ‘‰ [http://localhost:8000/](http://localhost:8000/)

##  ğŸ“¬ Feedback & Contributions

ğŸ’¬ Request features or report issues:\
ğŸ‘‰ [GitHub Issues](https://github.com/freedomson/midinho/issues)


## ğŸ“„ License

Private Ollama is open source and respects your digital freedom.
Use it. Hack it. Share it.
